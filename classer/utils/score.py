'''
Utilities for scoring the performance of models on datasets.
'''

from random import shuffle

import numpy as np

from status import set_status


def score_model(model, data, status_file, test_split=0.1, min_samples=10, iterations=3):
    '''Calculate a score for a model and a specific dataset
    Runs a configurable amount of test iterations and
    averages all results
    '''

    all_iterations = []

    for iteration in range(0, iterations):

        status_message = 'Processing Scoring Iteration {num}'.format(
                            num=iteration+1)
        set_status(status_file, status_message)
        shuffle(data)

        # Early exit for not enough samples
        if len(data) < min_samples:
            raise ValueError('A minimum number of {num} samples are '
                             'required to score a model'.format(num=min_samples))

        test_split_index = int(len(data) * test_split)
        train_data = data[test_split_index:]
        test_data = data[:test_split_index]

        # Train Model
        #TODO - make sure this doesn't carry over from previous iterations
        model.train(train_data)

        # Score
        iteration_stats, label_map = get_confusion(model, test_data)
        all_iterations.append(iteration_stats)

    overall_stats = {}
    list_stats = {}

    for single_iteration in all_iterations:
        for it_stat, it_confusion in single_iteration.iteritems():

            if it_stat not in overall_stats.keys():
                overall_stats[it_stat] = it_confusion
            else:
                overall_stats[it_stat] = overall_stats[it_stat] + it_confusion

    full_stats = calculate_full_stats(overall_stats, label_map)

    for stat_name, stat_data in full_stats.iteritems():
        if type(stat_data) not in [list, dict, basestring, unicode]:
            list_stats[stat_name] = stat_data.tolist()
        else:
            list_stats[stat_name] = stat_data

    list_stats['label_map'] = label_map

    return list_stats


def get_confusion(model, test_data, threshold=0.7):
    '''
    Calculate scores for the predictions generated by
    a trained model on a test set
    '''

    test_samples = [example['text'] for example in test_data]
    test_labels = [example['label'] for example in test_data]

    predictions = model.predict(test_samples)

    # labels ~ {"neg": 0, "pos": 1}
    label_map = model.label_mapper.label_map['forward']

    num_labels = len(label_map.keys())
    unknown_col = num_labels # for threshold confusion matrix

    # A confusion matrix is created, where the value at X, Y
    # indicates the strength of a prediction with true label X and
    # predicted label Y
    confusion = np.zeros((num_labels, num_labels))
    max_confusion = np.zeros((num_labels, num_labels))
    # Add an extra column in the threshold confusion matrix for
    # predictions that don't meet either threshold
    threshold_confusion = np.zeros((num_labels, num_labels+1))

    for prediction_idx, prediction in enumerate(predictions):

        correct_label = test_labels[prediction_idx]
        correct_label_index = label_map.get(correct_label)

        max_prediction = None
        max_prediction_score = 0.0

        for predicted_label in prediction.keys():
            predicted_label_index = label_map.get(predicted_label)

            # Populate confusion matrix
            confusion[correct_label_index, predicted_label_index] = \
                confusion[correct_label_index, predicted_label_index] + \
                prediction.get(predicted_label)

            prediction_score = prediction.get(predicted_label)
            if prediction_score > max_prediction_score:
                max_prediction = predicted_label
                max_prediction_score = prediction_score

        # Populate max confusion matrix
        max_prediction_index = label_map.get(max_prediction)
        max_confusion[correct_label_index, max_prediction_index] = \
            max_confusion[correct_label_index, max_prediction_index] + 1

        # Populate threshold confusion matrix
        if max_prediction_score >= threshold:
            threshold_confusion[correct_label_index, max_prediction_index] = \
                threshold_confusion[correct_label_index, max_prediction_index] + 1
        else:
            threshold_confusion[correct_label_index, unknown_col] = \
                threshold_confusion[correct_label_index, unknown_col] + 1

    stats = {
        "confusion": confusion,
        "max_confusion": max_confusion,
        "threshold_confusion": threshold_confusion
    }

    return stats, label_map


def calculate_full_stats(confusion_stats, label_map):
    """ Calculate Full precision + Recall stats given the
    total, max, and threshold confusion matrices
    """

    inverse_label_map = {}
    for label_name, label_index in label_map.iteritems():
        inverse_label_map[label_index] = label_name

    total_confusion = confusion_stats['confusion']
    total_precision, total_recall, total_f = get_precision_recall_f(
                                        total_confusion, inverse_label_map)
    confusion_stats['total_precision'] = total_precision
    confusion_stats['total_recall'] = total_recall
    confusion_stats['total_f'] = total_f

    max_confusion = confusion_stats['max_confusion']
    max_precision, max_recall, max_f = get_precision_recall_f(
                                        max_confusion, inverse_label_map)
    confusion_stats['max_precision'] = max_precision
    confusion_stats['max_recall'] = max_recall
    confusion_stats['max_f'] = max_f

    threshold_confusion = confusion_stats['threshold_confusion']
    threshold_precision, threshold_recall, threshold_f = get_precision_recall_f(
                                                threshold_confusion,
                                                inverse_label_map)

    confusion_stats['threshold_precision'] = threshold_precision
    confusion_stats['threshold_recall'] = threshold_recall
    confusion_stats['threshold_f'] = threshold_f

    return confusion_stats


def get_precision_recall_f(confusion, inverse_label_map):
    """ Calculate precision, recall, and F-score stats for
    a single confusion matrix
    """

    precision, recall, f_score = {}, {}, {}

    for label_index in range(len(inverse_label_map.keys())):
        label_name = inverse_label_map.get(label_index)

        true_count = confusion[label_index, label_index]
        all_predicted = confusion[:,label_index].sum()
        all_truth = confusion[label_index].sum()

        label_precision = true_count / all_predicted
        label_recall = true_count / all_truth

        precision[label_name] = label_precision
        recall[label_name] = label_recall

        f_score[label_name] = (2 * label_precision * label_recall) / \
                              (label_precision + label_recall)

    return precision, recall, f_score
